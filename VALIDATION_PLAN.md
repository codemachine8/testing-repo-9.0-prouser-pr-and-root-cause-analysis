# Flaky Test Autopilot - Validation Plan

This document outlines the complete validation strategy for the Flaky Test Autopilot system.

## üéØ Validation Goals

1. **Import Parsing**: Verify dependency tracking works across multiple languages
2. **Hash Invalidation**: Verify hashes change when code changes
3. **Flaky Detection**: Verify accurate detection of intermittent failures
4. **Multi-Framework**: Verify support for Jest, Pytest, and future frameworks
5. **Performance**: Verify system handles large test suites efficiently
6. **PR Quality**: Verify AI-generated fixes actually solve flakiness

## üìã Test Matrix

| Category | Test Count | Frameworks | Expected Flaky | Expected Stable |
|----------|-----------|------------|----------------|-----------------|
| With Dependencies | 6 | Jest, Pytest | 3 | 3 |
| Timing Patterns | 2 | Jest | 2 | 0 |
| Async Patterns | 2 | Jest | 1 | 1 |
| Random Data | 3 | Jest | 2 | 1 |
| External Deps | 2 | Jest | 2 | 0 |
| Original Tests | 14 | Jest, Pytest | 7 | 7 |
| **TOTAL** | **29** | **2** | **17** | **12** |

## üî¨ Phase 1: Initial CI Run (Day 1)

### Objectives:
- Verify XML generation
- Verify import parsing
- Verify hash calculation
- Verify API submission

### Steps:
1. Push all test code to GitHub
2. Trigger CI workflow
3. Review CI logs

### Expected Results:
```
üì¶ Found 2 test result file(s)
‚úÖ Parsed 29 test(s)
üîó Found dependencies for 9 tests
üì§ Sending 29 test results to API...
‚úÖ Results sent successfully
```

### Validation Checklist:
- [ ] Both `junit.xml` and `pytest-junit.xml` created
- [ ] 29 tests parsed total
- [ ] Tests with dependencies show import count
- [ ] Each test has unique hash
- [ ] All tests sent to API
- [ ] No errors in CI logs

## üî¨ Phase 2: Hash Invalidation Test (Day 1)

### Objectives:
- Verify hash changes when dependency changes
- Verify hash stays same when unrelated code changes

### Steps:

**Test 1: Modify Dependency**
```bash
# Run 1: Baseline
git push

# Run 2: Modify dependency
echo "// CHANGED" >> src/auth-helper.js
git add src/auth-helper.js
git commit -m "Test: Modify auth-helper"
git push

# Run 3: Revert
git revert HEAD
git push
```

**Test 2: Modify Unrelated Code**
```bash
# Run 4: Modify unrelated file
echo "// CHANGED" >> src/database.js
git add src/database.js
git commit -m "Test: Modify database"
git push
```

### Expected Results:

| Run | File Modified | Hash for `auth-with-deps.test.js` | Hash for `database-with-deps.test.js` |
|-----|--------------|-----------------------------------|---------------------------------------|
| 1 | None | `abc123...` | `def456...` |
| 2 | auth-helper.js | `xyz789...` ‚úÖ CHANGED | `def456...` ‚úÖ SAME |
| 3 | (reverted) | `abc123...` ‚úÖ SAME AS RUN 1 | `def456...` ‚úÖ SAME |
| 4 | database.js | `abc123...` ‚úÖ SAME | `ghi999...` ‚úÖ CHANGED |

### Validation Checklist:
- [ ] Hash changes when dependency changes
- [ ] Hash reverts when dependency reverts
- [ ] Hash stays same when unrelated code changes
- [ ] Only affected tests have hash changes

## üî¨ Phase 3: Flaky Detection (Days 1-3)

### Objectives:
- Collect sufficient data to detect flaky tests
- Verify pass rate calculations
- Distinguish flaky from broken tests

### Steps:
1. Run CI workflow 15 times (use workflow_dispatch)
   ```bash
   # Trigger manually 15 times via GitHub Actions UI
   # Or push empty commits
   for i in {1..15}; do
     git commit --allow-empty -m "Data collection run $i"
     git push
   done
   ```

2. After 15 runs, check dashboard

### Expected Results:

**Flaky Tests (17 tests):**
| Test Name | Expected Pass Rate | Category |
|-----------|-------------------|----------|
| `test_email_validation_flaky` | 65-75% | Random condition |
| `test_api_call_with_retry` | 65-75% | Network |
| `test_user_update_flaky` | 55-65% | State pollution |
| `test_race_condition_flaky` | 45-55% | Timing |
| `test_timeout_flaky` | 40-60% | Timing |
| `test_missing_await_flaky` | 30-70% | Async |
| `test_random_number_flaky` | 45-55% | Random |
| `test_random_array_flaky` | 20-40% | Random |
| `test_network_call_flaky` | 65-75% | Network |
| `test_file_system_flaky` | 60-80% | Filesystem |
| `test_flaky_with_import` | 65-75% | Random (Python) |
| `test_user_service_flaky` | 40-60% | State (Python) |
| [Existing flaky tests] | Various | Various |

**Stable Tests (12 tests):**
- Should have 100% pass rate
- Should NOT be flagged as flaky

### Validation Checklist:
- [ ] At least 15 runs completed
- [ ] Dashboard shows all 29 tests
- [ ] Flaky tests identified correctly
- [ ] Pass rates accurate (¬±5%)
- [ ] Stable tests not flagged
- [ ] Broken tests have 0% pass rate
- [ ] Test history visible in dashboard

## üî¨ Phase 4: Multi-Framework (Day 2)

### Objectives:
- Verify Jest tests work correctly
- Verify Pytest tests work correctly
- Verify import parsing for both languages

### Expected Results:

**Jest Tests:**
- 26 tests total
- Import parsing shows:
  - `auth-with-deps.test.js` ‚Üí 1 import
  - `api-with-deps.test.js` ‚Üí 1 import
  - `database-with-deps.test.js` ‚Üí 1 import

**Pytest Tests:**
- 3 tests total
- Import parsing shows:
  - `test_with_imports.py` ‚Üí 1 import (helpers.py)

### Validation Checklist:
- [ ] Jest XML parsed correctly
- [ ] Pytest XML parsed correctly
- [ ] JavaScript imports parsed
- [ ] Python imports parsed
- [ ] All tests from both frameworks in dashboard

## üî¨ Phase 5: PR Quality Evaluation (Days 3-7)

### Objectives:
- Evaluate AI-generated fix quality
- Measure fix success rate
- Identify improvement areas

### Steps:
1. Wait for AI to generate fix PRs
2. Review each PR for:
   - Problem identification accuracy
   - Fix correctness
   - Code quality
   - Test coverage

3. Merge fixes and re-run tests
4. Measure if flakiness reduced

### Expected Fix Categories:

**High Confidence Fixes:**
- Missing `await` keywords ‚Üí Add await
- Random data ‚Üí Add deterministic seed
- State pollution ‚Üí Add cleanup/setup

**Medium Confidence Fixes:**
- Race conditions ‚Üí Add synchronization
- Timing issues ‚Üí Add proper waits

**Low Confidence Fixes:**
- Network flakiness ‚Üí Add retries
- External dependencies ‚Üí Add mocks

### Validation Checklist:
- [ ] PRs created for detectable issues
- [ ] Fix descriptions are accurate
- [ ] Fixes actually work
- [ ] Code quality acceptable
- [ ] No regressions introduced
- [ ] Test passes consistently after fix

## üìä Success Metrics

### Critical Metrics (Must Pass):
- ‚úÖ XML generation: 100% success rate
- ‚úÖ Import parsing: 100% accuracy
- ‚úÖ Hash invalidation: Works correctly
- ‚úÖ API submission: 100% success rate
- ‚úÖ Flaky detection: >90% accuracy
- ‚úÖ False positives: <5%
- ‚úÖ CI stability: Never breaks due to tool

### Performance Metrics:
- Parse 29 tests: <3 seconds
- Calculate hashes: <2 seconds
- Send to API: <5 seconds
- Total overhead: <10 seconds

### Quality Metrics (After 15 runs):
- Flaky tests detected: 15-17 tests
- Pass rate accuracy: ¬±5%
- Stable tests flagged: 0 tests
- PR fixes that work: >70%

## üêõ Known Edge Cases to Test

1. **Empty test files**: Verify graceful handling
2. **Tests with no imports**: Hash should be file content only
3. **Circular imports**: Verify no infinite loops
4. **Missing dependency files**: Verify placeholder used
5. **Rate limit exceeded**: Verify graceful degradation
6. **Network failures**: Verify retry logic
7. **Large test suites**: Verify performance

## üìù Daily Checklist

### Day 1:
- [ ] Initial CI run successful
- [ ] All XMLs generated
- [ ] Import parsing working
- [ ] Hash invalidation test passed

### Day 2:
- [ ] 15 CI runs completed
- [ ] Flaky tests detected
- [ ] Dashboard accessible
- [ ] Data looks accurate

### Day 3:
- [ ] Review detected flaky tests
- [ ] Validate pass rates
- [ ] Check for false positives
- [ ] Document any issues

### Days 4-7:
- [ ] AI generates fix PRs
- [ ] Review PR quality
- [ ] Merge fixes
- [ ] Verify flakiness reduced

## üéì Final Report Template

After completing all phases:

```markdown
# Flaky Test Autopilot Validation Results

## Summary
- Tests run: X times
- Tests analyzed: 29
- Flaky tests detected: X
- False positives: X
- PRs generated: X
- Successful fixes: X

## Import Parsing
- Accuracy: X%
- Languages supported: JavaScript, Python
- Dependencies tracked: X files

## Hash Invalidation
- Works correctly: Yes/No
- False invalidations: X

## Flaky Detection
- True positives: X tests
- False positives: X tests
- Missed flaky tests: X tests
- Accuracy: X%

## PR Quality
- PRs created: X
- Fixes that worked: X (X%)
- Code quality: Good/Fair/Poor
- Areas for improvement: [list]

## Performance
- Average run time: X seconds
- Parse time: X seconds
- Hash calculation: X seconds
- API submission: X seconds

## Recommendations
1. [recommendation 1]
2. [recommendation 2]
3. [recommendation 3]
```

## üöÄ Ready to Start

Push this code to GitHub and begin Phase 1!

```bash
git add .
git commit -m "Add comprehensive E2E test suite for validation"
git push
```

Then watch the CI logs and follow the validation plan above.
